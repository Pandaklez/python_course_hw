{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание:** остроить  feed forward NN модель на pytorch для задачи NER из 4 дз. Разрешается использовать эмбеддинги. Необходимо побить бейзлайны"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача NER:\n",
    "\n",
    "Метрика качества f1 (f1_macro) (чем выше, тем лучше)\n",
    " \n",
    "baseline 1:            0.0604      random labels  \n",
    "baseline 2:            0.3966      PoS features + logistic regression  \n",
    "baseline 3:            0.8122      word2vec cbow embedding + baseline 2 + svm  \n",
    "мой результат из дз 4: 0.8577      CatBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "SEED=1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66874"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ner_short.csv', index_col=0)\n",
    "df.head(10)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence length\n",
    "tdf = df.set_index('sentence_idx')\n",
    "tdf['length'] = df.groupby('sentence_idx').tag.count()\n",
    "df = tdf.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorial variables\n",
    "le = LabelEncoder()\n",
    "df['pos'] = le.fit_transform(df.pos)\n",
    "df['next-pos'] = le.fit_transform(df['next-pos'])\n",
    "df['next-next-pos'] = le.fit_transform(df['next-next-pos'])\n",
    "df['prev-pos'] = le.fit_transform(df['prev-pos'])\n",
    "df['prev-prev-pos'] = le.fit_transform(df['prev-prev-pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 50155\n",
      "test 16719\n"
     ]
    }
   ],
   "source": [
    "# splitting\n",
    "y = LabelEncoder().fit_transform(df.tag)\n",
    "\n",
    "df_train, df_test, y_train, y_test = model_selection.train_test_split(df, y, stratify=y, \n",
    "                                                                      test_size=0.25, random_state=SEED, shuffle=True)\n",
    "print('train', df_train.shape[0])\n",
    "print('test', df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50155,), (16719,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y_train)\n",
    "ytest = np.array(y_test)\n",
    "y.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some wrappers to work with word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from collections import defaultdict\n",
    "\n",
    "   \n",
    "class Word2VecWrapper(TransformerMixin):\n",
    "    def __init__(self, window=5,negative=5, size=100, iter=100, is_cbow=False, random_state=SEED):\n",
    "        self.window_ = window\n",
    "        self.negative_ = negative\n",
    "        self.size_ = size\n",
    "        self.iter_ = iter\n",
    "        self.is_cbow_ = is_cbow\n",
    "        self.w2v = None\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def get_size(self):\n",
    "        return self.size_\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        X: list of strings\n",
    "        \"\"\"\n",
    "        sentences_list = [x.split() for x in X]\n",
    "        self.w2v = Word2Vec(sentences_list, \n",
    "                            window=self.window_,\n",
    "                            negative=self.negative_, \n",
    "                            size=self.size_, \n",
    "                            iter=self.iter_,\n",
    "                            sg=not self.is_cbow_, seed=self.random_state)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def has(self, word):\n",
    "        return word in self.w2v\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        X: a list of words\n",
    "        \"\"\"\n",
    "        if self.w2v is None:\n",
    "            raise Exception('model not fitted')\n",
    "        return np.array([self.w2v[w] if w in self.w2v else np.zeros(self.size_) for w in X ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Word2VecWrapper at 0x1d67a030b70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we exploit that word2vec is an unsupervised learning algorithm\n",
    "# so we can train it on the whole dataset (subject to discussion)\n",
    "\n",
    "sentences_list = [x.strip() for x in ' '.join(df.word).split('.')]\n",
    "\n",
    "w2v_cbow = Word2VecWrapper(window=5, negative=5, size=300, iter=300, is_cbow=True, random_state=SEED)\n",
    "w2v_cbow.fit(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "embeding = w2v_cbow\n",
    "encoder_pos = OneHotEncoder()\n",
    "X_train = sp.hstack([\n",
    "    embeding.transform(df_train.word),\n",
    "    embeding.transform(df_train['next-word']),\n",
    "    embeding.transform(df_train['next-next-word']),\n",
    "    embeding.transform(df_train['prev-word']),\n",
    "    embeding.transform(df_train['prev-prev-word']),\n",
    "    encoder_pos.fit_transform(df_train[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
    "])\n",
    "X_test = sp.hstack([\n",
    "    embeding.transform(df_test.word),\n",
    "    embeding.transform(df_test['next-word']),\n",
    "    embeding.transform(df_test['next-next-word']),\n",
    "    embeding.transform(df_test['prev-word']),\n",
    "    embeding.transform(df_test['prev-prev-word']),\n",
    "    encoder_pos.transform(df_test[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mtrand.RandomState at 0x1d600642900>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as tt\n",
    "from torch import nn\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.autograd import Variable\n",
    "seed = 15\n",
    "tt.manual_seed(seed)\n",
    "tt.cuda.manual_seed(seed)\n",
    "tt.backends.cudnn.deterministic=True\n",
    "np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1706\n",
    "hidden_size = 100\n",
    "num_classes = len(df.tag.value_counts(normalize=True))\n",
    "\n",
    "learning_rate = 0.1\n",
    "batch_size = 1000\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer\n",
    "        self.relu1 = nn.ReLU()                          # non-linearity\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) # 2nd Full-Connected Layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "    \n",
    "    def fit(self, X_train, y, batch_size, learning_rate, num_epochs):\n",
    "        # loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = tt.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        size = X_train.shape[0]\n",
    "        n_batches = int(np.ceil(size/batch_size))\n",
    "        print(n_batches)\n",
    "        for epoch in range(num_epochs):\n",
    "            # make random permutation over indices, use np.random.choice\n",
    "            indices = np.random.choice(size, size, replace=False)        \n",
    "            epoch_average_loss = 0\n",
    "            # iterate over mini-batches\n",
    "            for j in range(n_batches):\n",
    "                batch_idx = indices[j: j + batch_size]\n",
    "                \n",
    "                # we have to wrap data into tensors before feed them to neural network\n",
    "                # batch feature float tensor. use tt.from_numpy\n",
    "                batch_x = tt.from_numpy(X_train.toarray()[batch_idx])\n",
    "                # batch target long tensor. use tt.from_numpy\n",
    "                batch_y = tt.from_numpy(y[batch_idx])\n",
    "                #print(len(batch_x))\n",
    "                optimizer.zero_grad()\n",
    "                pred = model.forward(batch_x)\n",
    "                \n",
    "                # cross-entropy loss\n",
    "                loss = criterion(pred, batch_y.long())\n",
    "\n",
    "                # calculate gradients\n",
    "                loss.backward()\n",
    "                # make optimization step\n",
    "                optimizer.step()\n",
    "                epoch_average_loss += loss.data.detach().item()\n",
    "\n",
    "                # average loss for epoch\n",
    "                epoch_average_loss /= n_batches\n",
    "                # logging\n",
    "                if (j+1) % 50 == 0:\n",
    "                    print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                          %(epoch+1, num_epochs, j+1, len(X_train.toarray())//batch_size, epoch_average_loss))  #loss.data[0]))\n",
    "        return self\n",
    "            \n",
    "    def predict(self, X):\n",
    "        xt = tt.from_numpy(X.toarray())\n",
    "        pred = model.forward(xt)\n",
    "        pred = tt.softmax(pred, dim=-1)\n",
    "        pred = pred.detach().numpy()\n",
    "        predicted_y = np.argmax(pred, axis=1)\n",
    "        return predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous\n",
    "'''\n",
    "   def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer\n",
    "        self.relu = nn.ReLU()                          # ReLU Layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print('type X', type(x))\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FNN(\n",
       "  (fc1): Linear(in_features=1706, out_features=100, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc4): Linear(in_features=100, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instanciate\n",
    "model = FNN(input_size, hidden_size, num_classes)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [50/50], Loss: 0.0093\n",
      "Epoch [2/20], Step [50/50], Loss: 0.0080\n",
      "Epoch [3/20], Step [50/50], Loss: 0.0083\n",
      "Epoch [4/20], Step [50/50], Loss: 0.0072\n",
      "Epoch [5/20], Step [50/50], Loss: 0.0075\n",
      "Epoch [6/20], Step [50/50], Loss: 0.0056\n",
      "Epoch [7/20], Step [50/50], Loss: 0.0061\n",
      "Epoch [8/20], Step [50/50], Loss: 0.0053\n",
      "Epoch [9/20], Step [50/50], Loss: 0.0048\n",
      "Epoch [10/20], Step [50/50], Loss: 0.0042\n",
      "Epoch [11/20], Step [50/50], Loss: 0.0030\n",
      "Epoch [12/20], Step [50/50], Loss: 0.0025\n",
      "Epoch [13/20], Step [50/50], Loss: 0.0017\n",
      "Epoch [14/20], Step [50/50], Loss: 0.0023\n",
      "Epoch [15/20], Step [50/50], Loss: 0.0018\n",
      "Epoch [16/20], Step [50/50], Loss: 0.0016\n",
      "Epoch [17/20], Step [50/50], Loss: 0.0017\n",
      "Epoch [18/20], Step [50/50], Loss: 0.0014\n",
      "Epoch [19/20], Step [50/50], Loss: 0.0011\n",
      "Epoch [20/20], Step [50/50], Loss: 0.0013\n",
      "Wall time: 11min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FNN(\n",
       "  (fc1): Linear(in_features=1706, out_features=100, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc4): Linear(in_features=100, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y, batch_size, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predicting...\n",
      "train 0.31584474806737967\n",
      "test 0.2879993980908926\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('Starting predicting...')\n",
    "print('train', metrics.f1_score(y, model.predict(X_train), average='macro'))\n",
    "print('test', f1_score(y_test, model.predict(X_test), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FNN(\n",
       "  (fc1): Linear(in_features=1706, out_features=100, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc4): Linear(in_features=100, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1\n",
    "batch_size = 1000\n",
    "num_epochs = 50\n",
    "\n",
    "model = FNN(input_size, hidden_size, num_classes)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "Epoch [1/50], Step [50/50], Loss: 0.0047\n",
      "Epoch [2/50], Step [50/50], Loss: 0.0074\n",
      "Epoch [3/50], Step [50/50], Loss: 0.0035\n",
      "Epoch [4/50], Step [50/50], Loss: 0.0016\n",
      "Epoch [5/50], Step [50/50], Loss: 0.0013\n",
      "Epoch [6/50], Step [50/50], Loss: 0.0012\n",
      "Epoch [7/50], Step [50/50], Loss: 0.0007\n",
      "Epoch [8/50], Step [50/50], Loss: 0.0005\n",
      "Epoch [9/50], Step [50/50], Loss: 0.0013\n",
      "Epoch [10/50], Step [50/50], Loss: 0.0006\n",
      "Epoch [11/50], Step [50/50], Loss: 0.0003\n",
      "Epoch [12/50], Step [50/50], Loss: 0.0005\n",
      "Epoch [13/50], Step [50/50], Loss: 0.0003\n",
      "Epoch [14/50], Step [50/50], Loss: 0.0003\n",
      "Epoch [15/50], Step [50/50], Loss: 0.0004\n",
      "Epoch [16/50], Step [50/50], Loss: 0.0001\n",
      "Epoch [17/50], Step [50/50], Loss: 0.0001\n",
      "Epoch [18/50], Step [50/50], Loss: 0.0003\n",
      "Epoch [19/50], Step [50/50], Loss: 0.0002\n",
      "Epoch [20/50], Step [50/50], Loss: 0.0005\n",
      "Epoch [21/50], Step [50/50], Loss: 0.0001\n",
      "Epoch [22/50], Step [50/50], Loss: 0.0001\n",
      "Epoch [23/50], Step [50/50], Loss: 0.0002\n",
      "Epoch [24/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [25/50], Step [50/50], Loss: 0.0001\n",
      "Epoch [26/50], Step [50/50], Loss: 0.0002\n",
      "Epoch [27/50], Step [50/50], Loss: 0.0001\n",
      "Epoch [28/50], Step [50/50], Loss: 0.0003\n",
      "Epoch [29/50], Step [50/50], Loss: 0.0001\n",
      "Epoch [30/50], Step [50/50], Loss: 0.0001\n",
      "Epoch [31/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [32/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [33/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [34/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [35/50], Step [50/50], Loss: 0.0001\n",
      "Epoch [36/50], Step [50/50], Loss: 0.0002\n",
      "Epoch [37/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [38/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [39/50], Step [50/50], Loss: 0.0001\n",
      "Epoch [40/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [41/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [42/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [43/50], Step [50/50], Loss: 0.0001\n",
      "Epoch [44/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [45/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [46/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [47/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [48/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [49/50], Step [50/50], Loss: 0.0000\n",
      "Epoch [50/50], Step [50/50], Loss: 0.0000\n",
      "Wall time: 27min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FNN(\n",
       "  (fc1): Linear(in_features=1706, out_features=100, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc4): Linear(in_features=100, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y, batch_size, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predicting...\n",
      "train 0.6222747117098374\n",
      "test 0.4631345854475651\n",
      "accuracy test 0.938572881153179\n",
      "accuracy train 0.9573123317715083\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "print('Starting predicting...')\n",
    "print('train', metrics.f1_score(y, model.predict(X_train), average='macro'))\n",
    "print('test', f1_score(y_test, model.predict(X_test), average='macro'))\n",
    "print('accuracy test', accuracy_score(y_test, model.predict(X_test)))\n",
    "print('accuracy train', accuracy_score(y, model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом я побила **второй** бейзлайн."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FNN(\n",
       "  (fc1): Linear(in_features=1706, out_features=100, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc4): Linear(in_features=100, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1.5\n",
    "batch_size = 700\n",
    "num_epochs = 100\n",
    "\n",
    "model = FNN(input_size, hidden_size, num_classes)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "Epoch [1/100], Step [50/71], Loss: 0.0040\n",
      "Epoch [2/100], Step [50/71], Loss: 0.0003\n",
      "Epoch [3/100], Step [50/71], Loss: 0.0004\n",
      "Epoch [4/100], Step [50/71], Loss: 0.0003\n",
      "Epoch [5/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [6/100], Step [50/71], Loss: 0.0001\n",
      "Epoch [7/100], Step [50/71], Loss: 0.0001\n",
      "Epoch [8/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [9/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [10/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [11/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [12/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [13/100], Step [50/71], Loss: 0.0001\n",
      "Epoch [14/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [15/100], Step [50/71], Loss: 0.0002\n",
      "Epoch [16/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [17/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [18/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [19/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [20/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [21/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [22/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [23/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [24/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [25/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [26/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [27/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [28/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [29/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [30/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [31/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [32/100], Step [50/71], Loss: 0.0001\n",
      "Epoch [33/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [34/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [35/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [36/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [37/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [38/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [39/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [40/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [41/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [42/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [43/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [44/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [45/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [46/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [47/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [48/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [49/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [50/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [51/100], Step [50/71], Loss: 0.0001\n",
      "Epoch [52/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [53/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [54/100], Step [50/71], Loss: 0.0001\n",
      "Epoch [55/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [56/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [57/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [58/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [59/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [60/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [61/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [62/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [63/100], Step [50/71], Loss: 0.0001\n",
      "Epoch [64/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [65/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [66/100], Step [50/71], Loss: 0.0001\n",
      "Epoch [67/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [68/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [69/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [70/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [71/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [72/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [73/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [74/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [75/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [76/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [77/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [78/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [79/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [80/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [81/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [82/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [83/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [84/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [85/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [86/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [87/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [88/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [89/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [90/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [91/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [92/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [93/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [94/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [95/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [96/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [97/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [98/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [99/100], Step [50/71], Loss: 0.0000\n",
      "Epoch [100/100], Step [50/71], Loss: 0.0000\n",
      "Wall time: 1h 38min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FNN(\n",
       "  (fc1): Linear(in_features=1706, out_features=100, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc4): Linear(in_features=100, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y, batch_size, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predicting...\n",
      "train 0.7866016050000514\n",
      "test 0.6117913802227536\n",
      "accuracy test 0.9534063042047969\n",
      "accuracy train 0.9719070880271159\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "print('Starting predicting...')\n",
    "print('train', f1_score(y, model.predict(X_train), average='macro'))\n",
    "print('test', f1_score(y_test, model.predict(X_test), average='macro'))\n",
    "print('accuracy test', accuracy_score(y_test, model.predict(X_test)))\n",
    "print('accuracy train', accuracy_score(y, model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока 0.61 на тесте - мой лучший результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот результат **лучше**, но третий бейзлайн всё еще не побит. Я уже использовала эмбеддинги и делала много эпох. Попробуем еще увеличить learning rate и еще уменьшить размер батча."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FNN(\n",
       "  (fc1): Linear(in_features=1706, out_features=100, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc4): Linear(in_features=100, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1.5\n",
    "batch_size = 600\n",
    "num_epochs = 130\n",
    "\n",
    "model = FNN(input_size, hidden_size, num_classes)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "Epoch [1/130], Step [50/83], Loss: 0.0021\n",
      "Epoch [2/130], Step [50/83], Loss: 0.0003\n",
      "Epoch [3/130], Step [50/83], Loss: 0.0001\n",
      "Epoch [4/130], Step [50/83], Loss: 0.0001\n",
      "Epoch [5/130], Step [50/83], Loss: 0.0001\n",
      "Epoch [6/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [7/130], Step [50/83], Loss: 0.0001\n",
      "Epoch [8/130], Step [50/83], Loss: 0.0001\n",
      "Epoch [9/130], Step [50/83], Loss: 0.0001\n",
      "Epoch [10/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [11/130], Step [50/83], Loss: 0.0001\n",
      "Epoch [12/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [13/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [14/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [15/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [16/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [17/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [18/130], Step [50/83], Loss: 0.0003\n",
      "Epoch [19/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [20/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [21/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [22/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [23/130], Step [50/83], Loss: 0.0001\n",
      "Epoch [24/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [25/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [26/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [27/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [28/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [29/130], Step [50/83], Loss: 0.0001\n",
      "Epoch [30/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [31/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [32/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [33/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [34/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [35/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [36/130], Step [50/83], Loss: 0.0001\n",
      "Epoch [37/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [38/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [39/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [40/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [41/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [42/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [43/130], Step [50/83], Loss: 0.0002\n",
      "Epoch [44/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [45/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [46/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [47/130], Step [50/83], Loss: 0.0002\n",
      "Epoch [48/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [49/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [50/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [51/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [52/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [53/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [54/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [55/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [56/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [57/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [58/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [59/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [60/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [61/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [62/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [63/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [64/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [65/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [66/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [67/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [68/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [69/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [70/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [71/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [72/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [73/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [74/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [75/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [76/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [77/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [78/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [79/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [80/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [81/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [82/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [83/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [84/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [85/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [86/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [87/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [88/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [89/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [90/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [91/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [92/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [93/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [94/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [95/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [96/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [97/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [98/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [99/130], Step [50/83], Loss: 0.0001\n",
      "Epoch [100/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [101/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [102/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [103/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [104/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [105/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [106/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [107/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [108/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [109/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [110/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [111/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [112/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [113/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [114/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [115/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [116/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [117/130], Step [50/83], Loss: 0.0002\n",
      "Epoch [118/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [119/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [120/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [121/130], Step [50/83], Loss: 0.0001\n",
      "Epoch [122/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [123/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [124/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [125/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [126/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [127/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [128/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [129/130], Step [50/83], Loss: 0.0000\n",
      "Epoch [130/130], Step [50/83], Loss: 0.0000\n",
      "Wall time: 2h 25min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FNN(\n",
       "  (fc1): Linear(in_features=1706, out_features=100, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc4): Linear(in_features=100, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y, batch_size, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predicting...\n",
      "train 0.8388807129074735\n",
      "test 0.6282688477170282\n",
      "accuracy test 0.9552604820862491\n",
      "accuracy train 0.9751570132588974\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "print('Starting predicting...')\n",
    "print('train', f1_score(y, model.predict(X_train), average='macro'))\n",
    "print('test', f1_score(y_test, model.predict(X_test), average='macro'))\n",
    "print('accuracy test', accuracy_score(y_test, model.predict(X_test)))\n",
    "print('accuracy train', accuracy_score(y, model.predict(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.628 на тесте - это лучший результат. Я пыталась еще добавлять слои в нейронную сеть, но тогда первый же loss на трейне выходил nan. И параметры типа размер батча, количество эпох, learning rate я тоже уже покрутила, поэтому остановимся на 0.628. \n",
    "\n",
    "Два бейзлайна побиты."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
