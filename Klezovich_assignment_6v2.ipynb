{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54630f86578c16602698ce577de9eacb6aec12a1"
   },
   "source": [
    "# Assignment 6\n",
    "\n",
    "Develop RNN model in pytorch to solve the following problem:  \n",
    "    \n",
    "1. Detect sarcasm \n",
    "Data from https://www.kaggle.com/sherinclaudia/sarcastic-comments-on-reddit  \n",
    "Your quality metric = accuracy  \n",
    "Randomly select 20% of your data for test set. You can use it only for final perfomance estimation.   \n",
    " \n",
    "\n",
    "Remember, you can use GPU resourses in kaggle kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Апдейт: у меня получилось побить бейзлайн оставив знаки препинаня + прошлый самый удачный метод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['googlenewsvectorsnegative300', 'glove-twitter', 'sarcastic-comments-on-reddit']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../input/sarcastic-comments-on-reddit/train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "0489e8e66ea28c073e2ee0ca41d4a6942889f971"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                        ...                                                             parent_comment\n",
       "0      0                        ...                          Yeah, I get that argument. At this point, I'd ...\n",
       "1      0                        ...                          The blazers and Mavericks (The wests 5 and 6 s...\n",
       "2      0                        ...                                                    They're favored to win.\n",
       "3      0                        ...                                                 deadass don't kill my buzz\n",
       "4      0                        ...                          Yep can confirm I saw the tool they use for th...\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "2f8cdf672a465bc81889350d0d4a9f85ed36ed93"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "spacy_en.remove_pipe('tagger')\n",
    "spacy_en.remove_pipe('ner')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.lemma_ for tok in spacy_en.tokenizer(text)]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7d53ebdf609827616a20ad3d2551892d03a55ae3"
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import nltk\n",
    "import gensim\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.data import Field, LabelField, BucketIterator, ReversibleField, TabularDataset\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "3036cd933899d36e19a49d59b954f16a48206984"
   },
   "outputs": [],
   "source": [
    "TEXT = Field(include_lengths=True, batch_first=True, \n",
    "             tokenize=tokenizer,\n",
    "             eos_token='<eos>',\n",
    "             lower=True,\n",
    "             stop_words=nltk.corpus.stopwords.words('english')\n",
    "            )\n",
    "LABEL = LabelField(dtype=tt.int64, use_vocab=True)\n",
    "SCORE = Field(dtype=tt.int64, use_vocab=True)\n",
    "# label,comment,author,subreddit,score,ups,downs,date,created_utc,parent_comment\n",
    "dataset = TabularDataset('../input/sarcastic-comments-on-reddit/train-balanced-sarcasm.csv', format='csv', \n",
    "                         fields=[('label', LABEL),('comment', TEXT),('score', SCORE),(None, None),(None, None),\n",
    "                                 (None, None),(None, None),(None, None),(None, None),(None, None)],\n",
    "                         skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "0cce0f472b31121c266863a7383da66d00060b37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35493"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.build_vocab(dataset, min_freq=5)\n",
    "len(TEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "28b69ef1e39e231c9fa2dd4e83230d730014afac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<eos>', '.', ',', '-pron-', '?', '!', '\"', '...']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "dc3cfe87413e4cee5a3ae70869674edba7dbbcd0"
   },
   "outputs": [],
   "source": [
    "LABEL.build_vocab(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "9e96337fa24eb372532d783f02a8a8831cc673c2"
   },
   "outputs": [],
   "source": [
    "SCORE.build_vocab(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "f9042f6fbf0c9fde0de82071d5b14d50ea705620"
   },
   "outputs": [],
   "source": [
    "train, test = dataset.split(0.8, stratified=True)  # 20% for test\n",
    "train, valid = train.split(0.7, stratified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "8d411e70bc2e4bc84e2d0c99a9a3098ab8119574"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['0', '1'], dtype='<U1'), array([283031, 283031]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([x.label for x in train.examples], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "5e90f9ba1697a784ff3c97bf946b804b00c9e786"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['0', '1'], dtype='<U1'), array([121299, 121299]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([x.label for x in valid.examples], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "2c6be0ed188c6fcae772dd3239aeff01f8845520"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['0', '1'], dtype='<U1'), array([101083, 101083]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([x.label for x in test.examples], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "c319ba8c44fdaa47388705fef5e45a8840a4e917"
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=embed_size,\n",
    "                           hidden_size=hidden_size,\n",
    "                           bidirectional=True,\n",
    "                           batch_first=True,\n",
    "                          )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size * 2 *2, 3)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        x, x_lengths = batch.comment\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if x_lengths is not None:\n",
    "            x_lengths = x_lengths.view(-1).tolist()\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True)\n",
    "            \n",
    "        _, (hidden, cell) = self.rnn(x)\n",
    "        \n",
    "        hidden = hidden.transpose(0,1)\n",
    "        cell = cell.transpose(0,1)\n",
    "        hidden = hidden.contiguous().view(hidden.size(0),-1)\n",
    "        cell = cell.contiguous().view(cell.size(0),-1)\n",
    "        x = tt.cat([hidden, cell], dim=1).squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "7e11acc95721029589581f17e77d03b9a6415ba6"
   },
   "outputs": [],
   "source": [
    "tt.cuda.empty_cache()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "model = MyModel(len(TEXT.vocab.itos),\n",
    "                embed_size=100,\n",
    "                hidden_size=128,\n",
    "               )\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(batch_size, batch_size, batch_size),\n",
    "    shuffle=True,\n",
    "    sort_key=lambda x: len(x.comment),\n",
    "    sort_within_batch=True,\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True, cooldown=5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "3ea9f2f9b8fbc163c97e861f77270c2bc502e5ef"
   },
   "outputs": [],
   "source": [
    "def _train_epoch(model, iterator, optimizer, criterion, curr_epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        curr_loss = loss.data.cpu().detach().item()\n",
    "        \n",
    "        loss_smoothing = i / (i+1)\n",
    "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
    "\n",
    "        iterator.set_postfix(loss='%.5f' % running_loss)\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "def _test_epoch(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    with tt.no_grad():\n",
    "        for batch in iterator:\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, batch.label)\n",
    "            # loss = criterion(batch.label, pred)\n",
    "            epoch_loss += loss.data.item()\n",
    "\n",
    "    return epoch_loss / n_batches\n",
    "\n",
    "\n",
    "def nn_train(model, train_iterator, valid_iterator, criterion, optimizer, n_epochs=100,\n",
    "          scheduler=None, early_stopping=0):\n",
    "\n",
    "    prev_loss = 100500\n",
    "    es_epochs = 0\n",
    "    best_epoch = None\n",
    "    history = pd.DataFrame()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = _train_epoch(model, train_iterator, optimizer, criterion, epoch)\n",
    "        valid_loss = _test_epoch(model, valid_iterator, criterion)\n",
    "\n",
    "        valid_loss = valid_loss\n",
    "        print('validation loss %.5f' % valid_loss)\n",
    "\n",
    "        record = {'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss}\n",
    "        history = history.append(record, ignore_index=True)\n",
    "\n",
    "        if early_stopping > 0:\n",
    "            if valid_loss > prev_loss:\n",
    "                es_epochs += 1\n",
    "            else:\n",
    "                es_epochs = 0\n",
    "\n",
    "            if es_epochs >= early_stopping:\n",
    "                best_epoch = history[history.valid_loss == history.valid_loss.min()].iloc[0]\n",
    "                print('Early stopping! best epoch: %d val %.5f' % (best_epoch['epoch'], best_epoch['valid_loss']))\n",
    "                break\n",
    "\n",
    "            prev_loss = min(prev_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "b89bcccd45e03af3cd3efbe06865a49fe620c982"
   },
   "outputs": [],
   "source": [
    "#nn_train(model, train_iterator, valid_iterator, criterion, optimizer, scheduler=scheduler,\n",
    "#         n_epochs=2, early_stopping=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "a09ca191322a93cc1ac9924596457917cd5f62e7"
   },
   "outputs": [],
   "source": [
    "#for batch in test_iterator:\n",
    "#    pred = model(batch)\n",
    "#    pred = tt.softmax(pred, dim=1)\n",
    "#    pred = tt.argmax(pred, dim=1)\n",
    "    \n",
    "def eval(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    with tt.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch)  #.comment).squeeze(1)\n",
    "            pred = tt.softmax(predictions, dim=-1)\n",
    "            pred = pred.detach().numpy()\n",
    "            pred = np.argmax(pred, axis=1)\n",
    "            loss = criterion(batch.label, pred)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "107c0c2236da7d75df7f8583ebc67d0c058e5a3e"
   },
   "outputs": [],
   "source": [
    "# test\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "#print('Starting predicting...')\n",
    "#print('test', eval(model, test_iterator, accuracy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de89e9be87b5f586f910bf960778fd94e51ee65c"
   },
   "source": [
    "## With embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "916053c4a182118ba0f60437484cf0bc136df5c3"
   },
   "outputs": [],
   "source": [
    "import gensim.models.keyedvectors as word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "b5a97a02dd2374b5da43e04c462388bf979fed09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193515, 25)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file='../input/glove-twitter/glove.twitter.27B.25d.txt',\n",
    "               word2vec_output_file=\"gensim_glove_vectors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "fa38aeb583bf2112c29e1b0de9685921fef140aa"
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=True, encoding='utf-8', unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "edd33f393a367c6208dd68ad8b0dcd4a49f7a992"
   },
   "outputs": [],
   "source": [
    "#model0 = gensim.models.KeyedVectors.load_word2vec_format(\"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\",\n",
    "#                                                         binary=True, encoding='utf-8', unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "46c08cf7a9a80e604ff7d3d4f4f8953af5908f06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "embed_matrix = tt.FloatTensor(glove_model.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "bc5a0b28af25ccc487605af6c30f1ca54e0a8a8d"
   },
   "outputs": [],
   "source": [
    "class MyModel2(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(MyModel2, self).__init__()\n",
    "        #self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embed_matrix, freeze=True)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=embed_size,\n",
    "                           hidden_size=hidden_size,\n",
    "                           bidirectional=True,\n",
    "                           batch_first=True,\n",
    "                          )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size * 2 *2, 3)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        x, x_lengths = batch.comment\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if x_lengths is not None:\n",
    "            x_lengths = x_lengths.view(-1).tolist()\n",
    "            x = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True)\n",
    "            \n",
    "        _, (hidden, cell) = self.rnn(x)\n",
    "        \n",
    "        hidden = hidden.transpose(0,1)\n",
    "        cell = cell.transpose(0,1)\n",
    "        hidden = hidden.contiguous().view(hidden.size(0),-1)\n",
    "        cell = cell.contiguous().view(cell.size(0),-1)\n",
    "        x = tt.cat([hidden, cell], dim=1).squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "cd9c0d4bf245ce630fbfb9379b181f23b96a23e9"
   },
   "outputs": [],
   "source": [
    "tt.cuda.empty_cache()\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "model2 = MyModel2(len(TEXT.vocab.itos),\n",
    "                  embed_size=25,\n",
    "                  hidden_size=128)\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(batch_size, batch_size, batch_size),\n",
    "    shuffle=True,\n",
    "    sort_key=lambda x: len(x.comment),\n",
    "    sort_within_batch=True,\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model2.parameters())\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True, cooldown=5)\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "ddd92ccbed55eea40b803408e323f648c8253af4"
   },
   "outputs": [],
   "source": [
    "#nn_train(model2, train_iterator, valid_iterator, criterion, optimizer, scheduler=scheduler,\n",
    "#         n_epochs=2, early_stopping=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "1f9a78f55af354a30c0cb993c9e79a1cb42ac424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predicting...\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Starting predicting...')\n",
    "#print('test', eval(model2, test_iterator, accuracy_score))\n",
    "# 0.678 for model with pretrained word2vec\n",
    "# ? for model with pretrained glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3436568574292aae2a76501168215c322b31087b"
   },
   "source": [
    "## With gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "c29512d61376b22d4108d80fb3ef15373b4ef7dc"
   },
   "outputs": [],
   "source": [
    "def _train_epoch(model, iterator, optimizer, criterion, curr_epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch.label)\n",
    "        loss.backward()\n",
    "        tt.nn.utils.clip_grad_norm_(model.parameters(), 0.25)  # deprecation warning\n",
    "        optimizer.step()\n",
    "        \n",
    "        curr_loss = loss.data.cpu().detach().item()\n",
    "        \n",
    "        loss_smoothing = i / (i+1)\n",
    "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
    "\n",
    "        iterator.set_postfix(loss='%.5f' % running_loss)\n",
    "\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "028289e50e51d401294b2bcb726f71f6ad0ce5d8"
   },
   "outputs": [],
   "source": [
    "tt.cuda.empty_cache()\n",
    "\n",
    "batch_size = 32\n",
    "LR = 0.005\n",
    "\n",
    "model = MyModel(len(TEXT.vocab.itos),\n",
    "                embed_size=100,\n",
    "                hidden_size=128,\n",
    "               )\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(batch_size, batch_size, batch_size),\n",
    "    shuffle=True,\n",
    "    sort_key=lambda x: len(x.comment),\n",
    "    sort_within_batch=True,\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), amsgrad=True, betas=(0.9,0.98), eps=1e-9)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True, cooldown=5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "4a13f6640840d0236f3e30182b0d6c15589af63e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e322afa686d44099b9eaadd70b3da08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 0', max=17690, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_train(model, train_iterator, valid_iterator, criterion, optimizer, scheduler=scheduler,\n",
    "         n_epochs=3, early_stopping=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "764071fb9715bc821ff626dc53f876b76997f00d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predicting...\n",
      "test 0.7042707164234941\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Starting predicting...')\n",
    "print('test', eval(model, test_iterator, accuracy_score))\n",
    "# 0.688 for model with gradient clipping\n",
    "# 0.683 with gradient clipping & amsgrad=True and B2=0.98 and eps=1e-9\n",
    "# 0.681 with gradient clipping & amsgrad=True and B2=0.98 and eps=1e-9 and adding score column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2cd6dedcff57944f961d64fd31b1970bee00a3c3"
   },
   "source": [
    "Самый лучший результат 0.688 получился с gradient clipping и БЕЗ векторов, без усложнение Adam (с помощью amsgrad изменения В2 на 0.98 и eps=1e-9 и без добавления колонки score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5caa053e8af59c72b271189fece8faeedfb2ba4b"
   },
   "source": [
    "**Апдейт**  Я оставила знаки препинания и поставила на 3 эпохи, и получилось на тесте 0.704!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f6340144d578d159feb1bded16363c69bdc3b56"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
