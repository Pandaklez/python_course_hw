{
  "cells": [
    {
      "metadata": {
        "_uuid": "54630f86578c16602698ce577de9eacb6aec12a1"
      },
      "cell_type": "markdown",
      "source": "# Assignment 6\n\nDevelop RNN model in pytorch to solve the following problem:  \n    \n1. Detect sarcasm \nData from https://www.kaggle.com/sherinclaudia/sarcastic-comments-on-reddit  \nYour quality metric = accuracy  \nRandomly select 20% of your data for test set. You can use it only for final perfomance estimation.   \n \n\nRemember, you can use GPU resourses in kaggle kernels."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = pd.read_csv(\"../input/sarcastic-comments-on-reddit/train-balanced-sarcasm.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0489e8e66ea28c073e2ee0ca41d4a6942889f971"
      },
      "cell_type": "code",
      "source": "data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f8cdf672a465bc81889350d0d4a9f85ed36ed93"
      },
      "cell_type": "code",
      "source": "import spacy\n\n\nspacy_en = spacy.load('en')\nspacy_en.remove_pipe('tagger')\nspacy_en.remove_pipe('ner')\n\ndef tokenizer(text): # create a tokenizer function\n    return [tok.lemma_ for tok in spacy_en.tokenizer(text) if tok.text.isalpha()]            ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d53ebdf609827616a20ad3d2551892d03a55ae3"
      },
      "cell_type": "code",
      "source": "from sklearn.externals import joblib\nimport nltk\nimport gensim\nimport spacy\nfrom tqdm import tqdm_notebook\n\nfrom sklearn import metrics\n\nimport torch as tt\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torchtext.data import Field, LabelField, BucketIterator, ReversibleField, TabularDataset\n\n\n\nSEED = 42\nnp.random.seed(SEED)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3036cd933899d36e19a49d59b954f16a48206984"
      },
      "cell_type": "code",
      "source": "TEXT = Field(include_lengths=True, batch_first=True, \n             tokenize=tokenizer,\n             eos_token='<eos>',\n             lower=True,\n             stop_words=nltk.corpus.stopwords.words('english')\n            )\nLABEL = LabelField(dtype=tt.int64, use_vocab=True)\nSCORE = Field(dtype=tt.int64, use_vocab=True)\n# label,comment,author,subreddit,score,ups,downs,date,created_utc,parent_comment\ndataset = TabularDataset('../input/sarcastic-comments-on-reddit/train-balanced-sarcasm.csv', format='csv', \n                         fields=[('label', LABEL),('comment', TEXT),('score', SCORE),(None, None),(None, None),\n                                 (None, None),(None, None),(None, None),(None, None),(None, None)],\n                         skip_header=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0cce0f472b31121c266863a7383da66d00060b37"
      },
      "cell_type": "code",
      "source": "TEXT.build_vocab(dataset, min_freq=5)\nlen(TEXT.vocab.itos)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "28b69ef1e39e231c9fa2dd4e83230d730014afac"
      },
      "cell_type": "code",
      "source": "TEXT.vocab.itos[:10]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc3cfe87413e4cee5a3ae70869674edba7dbbcd0"
      },
      "cell_type": "code",
      "source": "LABEL.build_vocab(dataset)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e96337fa24eb372532d783f02a8a8831cc673c2"
      },
      "cell_type": "code",
      "source": "SCORE.build_vocab(dataset)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9042f6fbf0c9fde0de82071d5b14d50ea705620"
      },
      "cell_type": "code",
      "source": "train, test = dataset.split(0.8, stratified=True)  # 20% for test\ntrain, valid = train.split(0.7, stratified=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d411e70bc2e4bc84e2d0c99a9a3098ab8119574"
      },
      "cell_type": "code",
      "source": "np.unique([x.label for x in train.examples], return_counts=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e90f9ba1697a784ff3c97bf946b804b00c9e786"
      },
      "cell_type": "code",
      "source": "np.unique([x.label for x in valid.examples], return_counts=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c6be0ed188c6fcae772dd3239aeff01f8845520"
      },
      "cell_type": "code",
      "source": "np.unique([x.label for x in test.examples], return_counts=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c319ba8c44fdaa47388705fef5e45a8840a4e917"
      },
      "cell_type": "code",
      "source": "class MyModel(nn.Module):\n    \n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super(MyModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        \n        self.rnn = nn.LSTM(input_size=embed_size,\n                           hidden_size=hidden_size,\n                           bidirectional=True,\n                           batch_first=True,\n                          )\n        \n        self.fc = nn.Linear(hidden_size * 2 *2, 3)\n        \n    def forward(self, batch):\n        \n        x, x_lengths = batch.comment\n        \n        x = self.embedding(x)\n\n        if x_lengths is not None:\n            x_lengths = x_lengths.view(-1).tolist()\n            x = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True)\n            \n        _, (hidden, cell) = self.rnn(x)\n        \n        hidden = hidden.transpose(0,1)\n        cell = cell.transpose(0,1)\n        hidden = hidden.contiguous().view(hidden.size(0),-1)\n        cell = cell.contiguous().view(cell.size(0),-1)\n        x = tt.cat([hidden, cell], dim=1).squeeze(1)\n        x = self.fc(x)\n        return x",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e11acc95721029589581f17e77d03b9a6415ba6"
      },
      "cell_type": "code",
      "source": "tt.cuda.empty_cache()\n\nbatch_size = 32\n\nmodel = MyModel(len(TEXT.vocab.itos),\n                embed_size=100,\n                hidden_size=128,\n               )\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train, valid, test),\n    batch_sizes=(batch_size, batch_size, batch_size),\n    shuffle=True,\n    sort_key=lambda x: len(x.comment),\n    sort_within_batch=True,\n)\n\noptimizer = optim.Adam(model.parameters())\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True, cooldown=5)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\ncriterion = nn.CrossEntropyLoss()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ea9f2f9b8fbc163c97e861f77270c2bc502e5ef"
      },
      "cell_type": "code",
      "source": "def _train_epoch(model, iterator, optimizer, criterion, curr_epoch):\n\n    model.train()\n\n    running_loss = 0\n\n    n_batches = len(iterator)\n    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n\n    for i, batch in enumerate(iterator):\n        optimizer.zero_grad()\n\n        pred = model(batch)\n        loss = criterion(pred, batch.label)\n        loss.backward()\n        optimizer.step()\n\n        curr_loss = loss.data.cpu().detach().item()\n        \n        loss_smoothing = i / (i+1)\n        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n\n        iterator.set_postfix(loss='%.5f' % running_loss)\n\n    return running_loss\n\ndef _test_epoch(model, iterator, criterion):\n    model.eval()\n    epoch_loss = 0\n\n    n_batches = len(iterator)\n    with tt.no_grad():\n        for batch in iterator:\n            pred = model(batch)\n            loss = criterion(pred, batch.label)\n            # loss = criterion(batch.label, pred)\n            epoch_loss += loss.data.item()\n\n    return epoch_loss / n_batches\n\n\ndef nn_train(model, train_iterator, valid_iterator, criterion, optimizer, n_epochs=100,\n          scheduler=None, early_stopping=0):\n\n    prev_loss = 100500\n    es_epochs = 0\n    best_epoch = None\n    history = pd.DataFrame()\n\n    for epoch in range(n_epochs):\n        train_loss = _train_epoch(model, train_iterator, optimizer, criterion, epoch)\n        valid_loss = _test_epoch(model, valid_iterator, criterion)\n\n        valid_loss = valid_loss\n        print('validation loss %.5f' % valid_loss)\n\n        record = {'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss}\n        history = history.append(record, ignore_index=True)\n\n        if early_stopping > 0:\n            if valid_loss > prev_loss:\n                es_epochs += 1\n            else:\n                es_epochs = 0\n\n            if es_epochs >= early_stopping:\n                best_epoch = history[history.valid_loss == history.valid_loss.min()].iloc[0]\n                print('Early stopping! best epoch: %d val %.5f' % (best_epoch['epoch'], best_epoch['valid_loss']))\n                break\n\n            prev_loss = min(prev_loss, valid_loss)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b89bcccd45e03af3cd3efbe06865a49fe620c982"
      },
      "cell_type": "code",
      "source": "#nn_train(model, train_iterator, valid_iterator, criterion, optimizer, scheduler=scheduler,\n#         n_epochs=10, early_stopping=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a09ca191322a93cc1ac9924596457917cd5f62e7"
      },
      "cell_type": "code",
      "source": "#for batch in test_iterator:\n#    pred = model(batch)\n#    pred = tt.softmax(pred, dim=1)\n#    pred = tt.argmax(pred, dim=1)\n    \ndef eval(model, iterator, criterion):\n    epoch_loss = 0\n    model.eval()\n    with tt.no_grad():\n        for batch in iterator:\n            predictions = model(batch)  #.comment).squeeze(1)\n            pred = tt.softmax(predictions, dim=-1)\n            pred = pred.detach().numpy()\n            pred = np.argmax(pred, axis=1)\n            loss = criterion(batch.label, pred)\n            epoch_loss += loss.item()\n    return epoch_loss / len(iterator)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "107c0c2236da7d75df7f8583ebc67d0c058e5a3e"
      },
      "cell_type": "code",
      "source": "# test\nfrom sklearn.metrics import accuracy_score\n\nprint('Starting predicting...')\nprint('test', eval(model, test_iterator, accuracy_score))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "de89e9be87b5f586f910bf960778fd94e51ee65c"
      },
      "cell_type": "markdown",
      "source": "## With embeddings"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "916053c4a182118ba0f60437484cf0bc136df5c3"
      },
      "cell_type": "code",
      "source": "import gensim.models.keyedvectors as word2vec",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5a97a02dd2374b5da43e04c462388bf979fed09"
      },
      "cell_type": "code",
      "source": "from gensim.scripts.glove2word2vec import glove2word2vec\nglove2word2vec(glove_input_file='../input/glove-twitter/glove.twitter.27B.25d.txt',\n               word2vec_output_file=\"gensim_glove_vectors.txt\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa38aeb583bf2112c29e1b0de9685921fef140aa"
      },
      "cell_type": "code",
      "source": "from gensim.models.keyedvectors import KeyedVectors\nglove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=True, encoding='utf-8', unicode_errors='ignore')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "edd33f393a367c6208dd68ad8b0dcd4a49f7a992"
      },
      "cell_type": "code",
      "source": "#model0 = gensim.models.KeyedVectors.load_word2vec_format(\"../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\",\n#                                                         binary=True, encoding='utf-8', unicode_errors='ignore')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46c08cf7a9a80e604ff7d3d4f4f8953af5908f06"
      },
      "cell_type": "code",
      "source": "embed_matrix = tt.FloatTensor(glove_model.syn0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc5a0b28af25ccc487605af6c30f1ca54e0a8a8d"
      },
      "cell_type": "code",
      "source": "class MyModel2(nn.Module):\n    \n    def __init__(self, vocab_size, embed_size, hidden_size):\n        super(MyModel2, self).__init__()\n        #self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.embedding = nn.Embedding.from_pretrained(embed_matrix, freeze=True)\n        \n        self.rnn = nn.LSTM(input_size=embed_size,\n                           hidden_size=hidden_size,\n                           bidirectional=True,\n                           batch_first=True,\n                          )\n        \n        self.fc = nn.Linear(hidden_size * 2 *2, 3)\n        \n    def forward(self, batch):\n        \n        x, x_lengths = batch.comment\n        \n        x = self.embedding(x)\n\n        if x_lengths is not None:\n            x_lengths = x_lengths.view(-1).tolist()\n            x = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True)\n            \n        _, (hidden, cell) = self.rnn(x)\n        \n        hidden = hidden.transpose(0,1)\n        cell = cell.transpose(0,1)\n        hidden = hidden.contiguous().view(hidden.size(0),-1)\n        cell = cell.contiguous().view(cell.size(0),-1)\n        x = tt.cat([hidden, cell], dim=1).squeeze(1)\n        x = self.fc(x)\n        return x",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cd9c0d4bf245ce630fbfb9379b181f23b96a23e9"
      },
      "cell_type": "code",
      "source": "tt.cuda.empty_cache()\n\nbatch_size = 200\n\nmodel2 = MyModel2(len(TEXT.vocab.itos),\n                  embed_size=25,\n                  hidden_size=128)\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train, valid, test),\n    batch_sizes=(batch_size, batch_size, batch_size),\n    shuffle=True,\n    sort_key=lambda x: len(x.comment),\n    sort_within_batch=True,\n)\n\noptimizer = optim.Adam(model2.parameters())\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True, cooldown=5)\n#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\ncriterion = nn.CrossEntropyLoss()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ddd92ccbed55eea40b803408e323f648c8253af4"
      },
      "cell_type": "code",
      "source": "#nn_train(model2, train_iterator, valid_iterator, criterion, optimizer, scheduler=scheduler,\n#         n_epochs=2, early_stopping=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f9a78f55af354a30c0cb993c9e79a1cb42ac424"
      },
      "cell_type": "code",
      "source": "# test\nfrom sklearn.metrics import accuracy_score\n\nprint('Starting predicting...')\nprint('test', eval(model2, test_iterator, accuracy_score))\n# 0.678 for model with pretrained word2vec\n# ? for model with pretrained glove",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3436568574292aae2a76501168215c322b31087b"
      },
      "cell_type": "markdown",
      "source": "## With gradient clipping"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c29512d61376b22d4108d80fb3ef15373b4ef7dc"
      },
      "cell_type": "code",
      "source": "def _train_epoch(model, iterator, optimizer, criterion, curr_epoch):\n\n    model.train()\n\n    running_loss = 0\n\n    n_batches = len(iterator)\n    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n\n    for i, batch in enumerate(iterator):\n        optimizer.zero_grad()\n\n        pred = model(batch)\n        loss = criterion(pred, batch.label)\n        loss.backward()\n        tt.nn.utils.clip_grad_norm_(model.parameters(), 0.25)  # deprecation warning\n        optimizer.step()\n        \n        curr_loss = loss.data.cpu().detach().item()\n        \n        loss_smoothing = i / (i+1)\n        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n\n        iterator.set_postfix(loss='%.5f' % running_loss)\n\n    return running_loss",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "028289e50e51d401294b2bcb726f71f6ad0ce5d8"
      },
      "cell_type": "code",
      "source": "tt.cuda.empty_cache()\n\nbatch_size = 32\nLR = 0.005\n\nmodel = MyModel(len(TEXT.vocab.itos),\n                embed_size=100,\n                hidden_size=128,\n               )\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train, valid, test),\n    batch_sizes=(batch_size, batch_size, batch_size),\n    shuffle=True,\n    sort_key=lambda x: len(x.comment),\n    sort_within_batch=True,\n)\n\noptimizer = optim.Adam(model.parameters(), amsgrad=True, betas=(0.9,0.98), eps=1e-9)\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True, cooldown=5)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\ncriterion = nn.CrossEntropyLoss()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a13f6640840d0236f3e30182b0d6c15589af63e"
      },
      "cell_type": "code",
      "source": "nn_train(model, train_iterator, valid_iterator, criterion, optimizer, scheduler=scheduler,\n         n_epochs=3, early_stopping=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "764071fb9715bc821ff626dc53f876b76997f00d"
      },
      "cell_type": "code",
      "source": "# test\nfrom sklearn.metrics import accuracy_score\n\nprint('Starting predicting...')\nprint('test', eval(model, test_iterator, accuracy_score))\n# 0.688 for model with gradient clipping\n# 0.683 with gradient clipping & amsgrad=True and B2=0.98 and eps=1e-9\n# 0.681 with gradient clipping & amsgrad=True and B2=0.98 and eps=1e-9 and adding score column ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2cd6dedcff57944f961d64fd31b1970bee00a3c3"
      },
      "cell_type": "markdown",
      "source": "Самый лучший результат 0.688 получился с gradient clipping и БЕЗ векторов, без усложнение Adam (с помощью amsgrad изменения В2 на 0.98 и eps=1e-9 и без добавления колонки score)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5caa053e8af59c72b271189fece8faeedfb2ba4b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4f6340144d578d159feb1bded16363c69bdc3b56"
      },
      "cell_type": "markdown",
      "source": ""
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}